{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6edac92-c2e3-4fe8-99c2-f77e8b458682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "import colorsys\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import graphviz\n",
    "from statistics import mean, median, mode, stdev\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.rcParams['figure.constrained_layout.use'] = False\n",
    "SEED = 7355608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d29abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rubin_combine(vals, val_vars, log_normal=False):\n",
    "    \"\"\"Combines values from multiple imputations using Rubin's rules.\n",
    "    \n",
    "    Args:\n",
    "        vals: list of values to combine.\n",
    "        val_vars: list of within-imputation variances. Note that these should already be in log form if log_normal=True.\n",
    "        log_normal: Bool, set to True for log-normally distributed values (e.g. odds ratios)\n",
    "                   and False for normally distributed values\n",
    "    \"\"\"\n",
    "            \n",
    "    # Pool the values according to Rubin's rules\n",
    "    if log_normal:\n",
    "        transformed_vals = [math.log(val) for val in vals]\n",
    "        mean_val = mean(transformed_vals)\n",
    "        pooled_val = math.exp(mean_val)\n",
    "    else:\n",
    "        mean_val = mean(vals)\n",
    "        pooled_val = mean_val\n",
    "    \n",
    "    # Mean within-imputation variance\n",
    "    mean_imp_var = mean(val_vars)\n",
    "    \n",
    "    # Between-imputation variance\n",
    "    n_impute = len(vals)\n",
    "    if log_normal:\n",
    "        between_imp_var = np.sum((np.array(transformed_vals) - mean_val) ** 2) / (n_impute - 1)\n",
    "    else:\n",
    "        between_imp_var = np.sum((np.array(vals) - mean_val) ** 2) / (n_impute - 1)\n",
    "                                \n",
    "    # Total variance, combining within and between imputations\n",
    "    total_var = mean_imp_var + (1 + (1/n_impute))*between_imp_var\n",
    "                                \n",
    "    # Degrees of freedom (https://bookdown.org/mwheymans/bookmi/rubins-rules.html)\n",
    "    r = (between_imp_var + (between_imp_var/n_impute)) / mean_imp_var\n",
    "    nu = (n_impute - 1) * (1 + 1/r )**2\n",
    "    \n",
    "    alpha = 0.05\n",
    "    t_val = stats.t.ppf(1 - alpha/2, df=nu)  # t-distribution\n",
    "\n",
    "    if log_normal:\n",
    "        lower_ci = math.exp(mean_val - t_val * np.sqrt(total_var))\n",
    "        upper_ci = math.exp(mean_val + t_val * np.sqrt(total_var))\n",
    "    else:\n",
    "        lower_ci = mean_val - t_val * np.sqrt(total_var)\n",
    "        upper_ci = mean_val + t_val * np.sqrt(total_var)\n",
    "\n",
    "    return pooled_val, lower_ci, upper_ci, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90a6dae5-0477-4d52-b6d7-be92e6bfab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTCOME: response\n",
      "** Statistics for threshold values (gad7_sum) among 100 imputations: **\n",
      "Mean threshold: 10.6\n",
      "Median threshold: 10.5\n",
      "Mode threshold: 10.5\n",
      "Standard deviation of thresholds: 0.4380858271151806\n",
      "Minimum threshold: 10.5\n",
      "Maximum threshold: 12.5\n",
      "Unique thresholds: [10.5 12.5]\n",
      "Proportion of thresholds equal to mode=10.5: 0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Generate decision trees for interpretation, using two strategies: \n",
    "    1. Fit a decision tree to the entire dataset, concatenating all imputed versions\n",
    "    2. Fit a decision tree separately to each imputed version of the dataset (focusing on a variable of interest), and examine the distribution\n",
    "'''\n",
    "\n",
    "def man_cmap(cmap, value=1.):\n",
    "    colors = cmap(np.arange(cmap.N))\n",
    "    hls = np.array([colorsys.rgb_to_hls(*c) for c in colors[:,:3]])\n",
    "    hls[:,1] *= value\n",
    "    rgb = np.clip(np.array([colorsys.hls_to_rgb(*c) for c in hls]), 0,1)\n",
    "    return mcolors.LinearSegmentedColormap.from_list(\"\", rgb)\n",
    "\n",
    "pt_data = pd.read_csv(\"miceRanger_imputed_formatted_Brighten-v1_all.csv\")\n",
    "\n",
    "predictors = ['base_phq9', 'gad7_sum', 'age', 'sds_sum', 'alc_sum', 'gender', 'education', 'working', 'marital_status', 'race', 'income_satisfaction', 'mania_history', 'psychosis_history']\n",
    "\n",
    "for outcome in [\"response\"]:\n",
    "    \n",
    "    print(\"OUTCOME: \" + outcome)\n",
    "    \n",
    "    x = pt_data[predictors].copy()\n",
    "        \n",
    "    clf_all = tree.DecisionTreeClassifier(max_depth=1)\n",
    "    clf_all = clf_all.fit(x, pt_data[outcome])\n",
    "    clf_data = tree.export_graphviz(clf_all, out_file=None, feature_names=predictors, class_names=[\"no-\" + outcome, outcome])\n",
    "    graph = graphviz.Source(clf_data)\n",
    "    graph.render(\"decision_tree_\" + outcome)\n",
    "    \n",
    "    #http://www.futurile.net/2016/02/27/matplotlib-beautiful-plots-with-style/\n",
    "    plt.style.use('ggplot')\n",
    "    txt_col = 'k'\n",
    "    plt.rcParams['text.color'] = txt_col\n",
    "    plt.rcParams['axes.labelcolor'] = txt_col\n",
    "    plt.rcParams['xtick.color'] = txt_col\n",
    "    plt.rcParams['ytick.color'] = txt_col\n",
    "    plt.rcParams['axes.labelsize'] = 16\n",
    "    plt.rcParams['axes.labelweight'] = 'bold'\n",
    "    plt.rcParams['xtick.labelsize'] = 16\n",
    "    plt.rcParams['ytick.labelsize'] = 16\n",
    "    plt.rcParams['legend.fontsize'] = 16\n",
    "    plt.rcParams['figure.titlesize'] = 24\n",
    "    plt.tight_layout()\n",
    "\n",
    "    feature_of_interest = \"gad7_sum\"\n",
    "    verbose=False\n",
    "    thresholds = []\n",
    "    for imp in pt_data[\"_Imputation_\"].unique():\n",
    "        pt_data_imp = pt_data[pt_data[\"_Imputation_\"] == imp]\n",
    "\n",
    "        x = pt_data_imp[[feature_of_interest]].copy()\n",
    "            \n",
    "        clf_all = tree.DecisionTreeClassifier(max_depth=1)\n",
    "        clf_all = clf_all.fit(x, pt_data_imp[outcome])\n",
    "\n",
    "        # Get feature and threshold used by the tree\n",
    "        feature = clf_all.tree_.feature[0]  # Get feature index used at root node\n",
    "        threshold = clf_all.tree_.threshold[0]  # Get threshold value used at root node\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "        # Get class distributions for left and right nodes to determine relationship direction\n",
    "        left_dist = clf_all.tree_.value[1]  # Class distribution when feature <= threshold\n",
    "        right_dist = clf_all.tree_.value[2]  # Class distribution when feature > threshold\n",
    "        left_prob = left_dist[0][1] / left_dist[0].sum()  # Probability of response=True when <= threshold\n",
    "        right_prob = right_dist[0][1] / right_dist[0].sum()  # Probability of response=True when > threshold\n",
    "\n",
    "        if verbose:\n",
    "            direction = \"more\" if right_prob > left_prob else \"less\"\n",
    "            print(f\"\\nThreshold: {feature_of_interest} = {threshold:.2f}\")\n",
    "            print(f\"Values above threshold are {direction} likely to have response=True\")\n",
    "    \n",
    "    print(f\"** Statistics for threshold values ({feature_of_interest}) among {pt_data[\"_Imputation_\"].nunique()} imputations: **\")\n",
    "    print(f\"Mean threshold: {mean(thresholds)}\")\n",
    "    print(f\"Median threshold: {median(thresholds)}\")\n",
    "    print(f\"Mode threshold: {mode(thresholds)}\")\n",
    "    print(f\"Standard deviation of thresholds: {stdev(thresholds)}\")\n",
    "    print(f\"Minimum threshold: {min(thresholds)}\")\n",
    "    print(f\"Maximum threshold: {max(thresholds)}\")\n",
    "    print(f\"Unique thresholds: {np.unique(thresholds)}\")\n",
    "    print(f\"Proportion of thresholds equal to mode={mode(thresholds)}: {np.mean(np.array(thresholds) == mode(thresholds))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "295a6595-ae49-4c42-aac7-a7d56982e447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STUDY GROUP: all\n",
      "Condition: gad7_10_or_lower\n",
      "----OUTCOME: response----\n",
      "RUBIN OddsR: 3.135 [ 2.06 , 4.77 ].\n",
      "avg OddsR: 3.148\n",
      "based on  100  odds ratios\n",
      "---------\n",
      "Condition: gad7_above_10\n",
      "----OUTCOME: response----\n",
      "RUBIN OddsR: 0.319 [ 0.21 , 0.485 ].\n",
      "avg OddsR: 0.32\n",
      "based on  100  odds ratios\n",
      "---------\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "'''Generate odds ratios for treatment response (using a single-variable threshold of interest), pooling across imputed versions of the dataset using Rubin's rules. \n",
    "'''\n",
    "\n",
    "n_impute=100\n",
    "\n",
    "\n",
    "for group in [\"all\"]:\n",
    "    print(\"STUDY GROUP:\", group)\n",
    "\n",
    "    pt_data = pd.read_csv(f\"miceRanger_imputed_formatted_Brighten-v1_{group}.csv\")\n",
    "\n",
    "    pt_data = pt_data[[\"_Imputation_\", \"gad7_sum\", \"response\"]]\n",
    "\n",
    "    #pt_data[\"finish\"] = pt_data[\"bddybocs_tot_recalc_post\"].notnull()\n",
    "    pt_data[\"gad7_10_or_lower\"] = pt_data[\"gad7_sum\"] <= 10\n",
    "    pt_data[\"gad7_above_10\"] = pt_data[\"gad7_sum\"] > 10\n",
    "\n",
    "    for cond in [\"gad7_10_or_lower\", \"gad7_above_10\"]:\n",
    "        print(\"Condition: \" + cond)\n",
    "        for outcome in [\"response\"]:\n",
    "            print(\"----OUTCOME: \" + outcome + \"----\")\n",
    "\n",
    "            or_vals = []\n",
    "            or_vars = []\n",
    "            for imp in pt_data[\"_Imputation_\"].unique():\n",
    "                pt_data_imp = pt_data[pt_data[\"_Imputation_\"] == imp]\n",
    "                cont_table = np.array([\n",
    "                    [len(pt_data_imp[pt_data_imp[cond] & pt_data_imp[outcome]]), len(pt_data_imp[~pt_data_imp[cond] & pt_data_imp[outcome]])],\n",
    "                    [len(pt_data_imp[pt_data_imp[cond] & ~pt_data_imp[outcome]]), len(pt_data_imp[~pt_data_imp[cond] & ~pt_data_imp[outcome]])]\n",
    "                ])\n",
    "\n",
    "                # If any cell of the contingency table is zero, add 0.5 to all cells. \n",
    "                if np.any(cont_table == 0):\n",
    "                    cont_table = cont_table + 0.5\n",
    "\n",
    "                # Within-imputation value estimate\n",
    "                or_vals.append((cont_table[0,0]/cont_table[0,1]) / (cont_table[1,0]/cont_table[1,1]))\n",
    "\n",
    "                # Within-imputation variance\n",
    "                or_vars.append(sum([1/val for val in cont_table.reshape(-1)]))\n",
    "\n",
    "            pooled_or, lower_ci, upper_ci, ors = rubin_combine(or_vals, or_vars, log_normal=True)\n",
    "                                                            \n",
    "            print(\"RUBIN OddsR:\", round(pooled_or, 3), \"[\", round(lower_ci, 3), \",\", round(upper_ci, 3), \"].\")\n",
    "            print(\"avg OddsR:\", round(sum(ors)/len(ors), 3))\n",
    "            print(\"based on \", len(ors), \" odds ratios\")\n",
    "            \n",
    "        print(\"---------\")\n",
    "    print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75f2556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_gad7_sds_correlation(df):\n",
    "    \"\"\"\n",
    "    Calculate correlation between GAD-7 and SDS scores, handling missing values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing 'gad7_sum' and 'sds_sum' columns\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing correlation statistics and sample sizes\n",
    "    \"\"\"\n",
    "    # Remove rows where either GAD-7 or SDS is missing\n",
    "    complete_cases = df.dropna(subset=['gad7_sum', 'sds_sum'])\n",
    "    \n",
    "    # Calculate number of complete and missing cases\n",
    "    n_complete = len(complete_cases)\n",
    "    n_total = len(df)\n",
    "    n_missing_gad7 = df['gad7_sum'].isna().sum()\n",
    "    n_missing_sds = df['sds_sum'].isna().sum()\n",
    "    \n",
    "    # Calculate correlations if we have at least 2 complete cases\n",
    "    if n_complete >= 2:\n",
    "        pearson_r, pearson_p = stats.pearsonr(\n",
    "            complete_cases['gad7_sum'], \n",
    "            complete_cases['sds_sum']\n",
    "        )\n",
    "        spearman_r, spearman_p = stats.spearmanr(\n",
    "            complete_cases['gad7_sum'], \n",
    "            complete_cases['sds_sum']\n",
    "        )\n",
    "    else:\n",
    "        pearson_r = pearson_p = spearman_r = spearman_p = np.nan\n",
    "    \n",
    "    results = {\n",
    "        'n_total': n_total,\n",
    "        'n_complete': n_complete,\n",
    "        'n_missing_gad7': n_missing_gad7,\n",
    "        'n_missing_sds': n_missing_sds,\n",
    "        'pearson_r': pearson_r,\n",
    "        'pearson_p': pearson_p,\n",
    "        'spearman_r': spearman_r,\n",
    "        'spearman_p': spearman_p\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Complete cases: {results['n_complete']} out of {results['n_total']}\")\n",
    "    print(f\"Missing GAD-7: {results['n_missing_gad7']}\")\n",
    "    print(f\"Missing SDS: {results['n_missing_sds']}\")\n",
    "    print(f\"Pearson correlation: r = {results['pearson_r']:.3f}, p = {results['pearson_p']:.3f}\")\n",
    "    print(f\"Spearman correlation: ρ = {results['spearman_r']:.3f}, p = {results['spearman_p']:.3f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3fe2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete cases: 49300 out of 49300\n",
      "Missing GAD-7: 0\n",
      "Missing SDS: 0\n",
      "Pearson correlation: r = 0.553, p = 0.000\n",
      "Spearman correlation: ρ = 0.556, p = 0.000\n"
     ]
    }
   ],
   "source": [
    "pt_data = pd.read_csv(f\"miceRanger_imputed_formatted_Brighten-v1_{group}.csv\")\n",
    "\n",
    "results = analyze_gad7_sds_correlation(pt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fca0f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete cases: 474 out of 493\n",
      "Missing GAD-7: 18\n",
      "Missing SDS: 16\n",
      "Pearson correlation: r = 0.546, p = 0.000\n",
      "Spearman correlation: ρ = 0.547, p = 0.000\n"
     ]
    }
   ],
   "source": [
    "pt_data_missing = pd.read_csv(f\"missing_formatted_Brighten-v1_{group}.csv\")\n",
    "\n",
    "results = analyze_gad7_sds_correlation(pt_data_missing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
